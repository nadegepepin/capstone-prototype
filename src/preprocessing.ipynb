{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import PIL\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary_url = https://www.mit.edu/~ecprice/wordlist.10000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "         \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('../data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "400000\n"
     ]
    }
   ],
   "source": [
    "print(len(word_to_vec_map[\"sister\"]))\n",
    "print(len(word_to_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sentence(sentence, word_to_vec_map, Ty):\n",
    "    \"\"\"\n",
    "    Converts a sentence (string) into a list of words (strings). Extracts the GloVe representation of each word\n",
    "    and returns the array of vector encoding the meaning of the sentence.\n",
    "    \n",
    "    Arguments:\n",
    "    sentence -- string, one training example from X\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    Ty -- the max length of the sentence. The sentence is truncated in case its size is greater than Ty.\n",
    "    \n",
    "    Returns:\n",
    "    embedding -- return a numpy-array of shape (Ty, 50). When the sentence size is < Ty, the array is padded with 50-dimensional zero vectors.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Add a \".\" at the end of the sentence. It is our <EOS> element for now // FIX ME\n",
    "    p = re.compile('.*(\\.)$')\n",
    "    if not (p.match(sentence)):\n",
    "        sentence += \".\"\n",
    "    \n",
    "    # Step 2: Split the sentence into a list of lower case tokens\n",
    "    words = [i.lower() for i in word_tokenize(sentence)]   \n",
    "    sentence_size = len(words)\n",
    "    \n",
    "    # Initialize the embedding.\n",
    "    embedding = np.zeros((Ty,50))\n",
    "    \n",
    "    # Step 3: Loops over the token list.\n",
    "    i = 0\n",
    "    for idx, w in enumerate(words):\n",
    "        #The sentence is truncated whenlarger than Ty. The <EOS> character \".\" is added back.\n",
    "        if (i == Ty - 1):\n",
    "            embedding[i][:] = word_to_vec_map[\".\"] \n",
    "            break\n",
    "        try:\n",
    "            embedding[i][:] = word_to_vec_map[w]\n",
    "            i += 1\n",
    "        except KeyError:\n",
    "            # Ignores unknown words for now\n",
    "            continue\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.5323e-01,  5.9811e-02, -1.0577e-01, -3.3300e-01,  7.2359e-01,\n",
       "        -8.7170e-02, -6.1053e-01, -3.7695e-02, -3.0945e-01,  2.1805e-01,\n",
       "        -4.3605e-01,  4.7318e-01, -7.6866e-01, -2.7130e-01,  1.1042e+00,\n",
       "         5.9141e-01,  5.6962e-01, -1.8678e-01,  1.4867e-01, -6.7292e-01,\n",
       "        -3.4672e-01,  5.2284e-01,  2.2959e-01, -7.2014e-02,  9.3967e-01,\n",
       "        -2.3985e+00, -1.3238e+00,  2.8698e-01,  7.5509e-01, -7.6522e-01,\n",
       "         3.3425e+00,  1.7233e-01, -5.1803e-01, -8.2970e-01, -2.9333e-01,\n",
       "        -5.0076e-01, -1.5228e-01,  9.8973e-02,  1.8146e-01, -1.7420e-01,\n",
       "        -4.0666e-01,  2.0348e-01, -1.1788e-02,  4.8252e-01,  2.4598e-02,\n",
       "         3.4064e-01, -8.4724e-02,  5.3240e-01, -2.5103e-01,  6.2546e-01],\n",
       "       [ 2.1705e-01,  4.6515e-01, -4.6757e-01,  1.0082e-01,  1.0135e+00,\n",
       "         7.4845e-01, -5.3104e-01, -2.6256e-01,  1.6812e-01,  1.3182e-01,\n",
       "        -2.4909e-01, -4.4185e-01, -2.1739e-01,  5.1004e-01,  1.3448e-01,\n",
       "        -4.3141e-01, -3.1230e-02,  2.0674e-01, -7.8138e-01, -2.0148e-01,\n",
       "        -9.7401e-02,  1.6088e-01, -6.1836e-01, -1.8504e-01, -1.2461e-01,\n",
       "        -2.2526e+00, -2.2321e-01,  5.0430e-01,  3.2257e-01,  1.5313e-01,\n",
       "         3.9636e+00, -7.1365e-01, -6.7012e-01,  2.8388e-01,  2.1738e-01,\n",
       "         1.4433e-01,  2.5926e-01,  2.3434e-01,  4.2740e-01, -4.4451e-01,\n",
       "         1.3813e-01,  3.6973e-01, -6.4289e-01,  2.4142e-02, -3.9315e-02,\n",
       "        -2.6037e-01,  1.2017e-01, -4.3782e-02,  4.1013e-01,  1.7960e-01],\n",
       "       [ 2.3533e-01,  9.1320e-01, -1.2008e+00,  6.5595e-03,  1.2843e+00,\n",
       "        -1.0495e-01, -4.6928e-01, -1.6064e-01, -2.3046e-03,  8.8219e-01,\n",
       "        -5.8227e-01,  4.9887e-01,  2.8537e-01,  2.1317e-02,  7.0205e-01,\n",
       "        -2.4932e-01,  6.3965e-01,  6.8058e-01, -9.6489e-02, -6.9284e-01,\n",
       "        -1.4633e-01,  1.1343e+00, -1.5865e-01, -3.4477e-02,  1.4644e+00,\n",
       "        -4.6223e-01, -1.3821e+00,  4.5211e-01,  9.9220e-01, -5.5959e-01,\n",
       "         2.0021e+00,  5.4034e-01, -8.0539e-02, -2.1476e-02,  2.6856e-02,\n",
       "         4.2905e-01, -1.1890e-01,  6.9764e-01, -2.9882e-02, -5.8077e-01,\n",
       "         2.0491e-01,  9.5888e-02, -3.9682e-01, -1.8709e-01,  2.1904e-01,\n",
       "         5.7694e-01,  6.6142e-02, -1.6162e-01, -2.4670e-02,  8.4626e-01],\n",
       "       [ 4.8251e-01,  8.7746e-01, -2.3455e-01,  2.6200e-02,  7.9691e-01,\n",
       "         4.3102e-01, -6.0902e-01, -6.0764e-01, -4.2812e-01, -1.2523e-02,\n",
       "        -1.2894e+00,  5.2656e-01, -8.2763e-01,  3.0689e-01,  1.1972e+00,\n",
       "        -4.7674e-01, -4.6885e-01, -1.9524e-01, -2.8403e-01,  3.5237e-01,\n",
       "         4.5536e-01,  7.6853e-01,  6.2157e-03,  5.5421e-01,  1.0006e+00,\n",
       "        -1.3973e+00, -1.6894e+00,  3.0003e-01,  6.0678e-01, -4.6044e-01,\n",
       "         2.5961e+00, -1.2178e+00,  2.8747e-01, -4.6175e-01, -2.5943e-01,\n",
       "         3.8209e-01, -2.8312e-01, -4.7642e-01, -5.9444e-02, -5.9202e-01,\n",
       "         2.5613e-01,  2.1306e-01, -1.6129e-02, -2.9873e-01, -1.9468e-01,\n",
       "         5.3611e-01,  7.5459e-01, -4.1120e-01,  2.3625e-01,  2.6451e-01],\n",
       "       [ 1.5164e-01,  3.0177e-01, -1.6763e-01,  1.7684e-01,  3.1719e-01,\n",
       "         3.3973e-01, -4.3478e-01, -3.1086e-01, -4.4999e-01, -2.9486e-01,\n",
       "         1.6608e-01,  1.1963e-01, -4.1328e-01, -4.2353e-01,  5.9868e-01,\n",
       "         2.8825e-01, -1.1547e-01, -4.1848e-02, -6.7989e-01, -2.5063e-01,\n",
       "         1.8472e-01,  8.6876e-02,  4.6582e-01,  1.5035e-02,  4.3474e-02,\n",
       "        -1.4671e+00, -3.0384e-01, -2.3441e-02,  3.0589e-01, -2.1785e-01,\n",
       "         3.7460e+00,  4.2284e-03, -1.8436e-01, -4.6209e-01,  9.8329e-02,\n",
       "        -1.1907e-01,  2.3919e-01,  1.1610e-01,  4.1705e-01,  5.6763e-02,\n",
       "        -6.3681e-05,  6.8987e-02,  8.7939e-02, -1.0285e-01, -1.3931e-01,\n",
       "         2.2314e-01, -8.0803e-02, -3.5652e-01,  1.6413e-02,  1.0216e-01]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_sentence(\"What a wonderful story, can't wait\", word_to_vec_map, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(articles)\n",
    "print(dictionary)\n",
    "\n",
    "# Select the id for \"computer\": computer_id\n",
    "computer_id = dictionary.token2id.get(\"computer\")\n",
    "\n",
    "# Use computer_id with the dictionary to print the word\n",
    "print(dictionary.get(computer_id))\n",
    "\n",
    "# Create a MmCorpus: corpus\n",
    "corpus = [dictionary.doc2bow(article) for article in articles]\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from the fifth document\n",
    "print(corpus[4][:10])\n",
    "\n",
    "# Save the fifth document: doc\n",
    "doc = corpus[4]\n",
    "\n",
    "# Sort the doc for frequency: bow_doc\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words of the document alongside the count\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "    \n",
    "# Create the defaultdict: total_word_count\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count\n",
    "    \n",
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print(tfidf_weights[:5])\n",
    "\n",
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary.get(term_id), weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
